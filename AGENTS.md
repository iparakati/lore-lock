# AGENTS.md: Living Documentation

> **Note:** This is a living document. It should be continually improved and refined to support the future development of the project. Developers and Agents are encouraged to update this file with summarized insights from simulation playtesting, architectural decisions, and best practices discovered during implementation.

# Inform 7-Style Python Engine Architecture

This document outlines the architecture for the Python-based Interactive Fiction engine generated by `compiler.py`. The goal is to closely mimic the core functionality of the Inform 7 (I7) Standard Rules, specifically focusing on the Object Model, Action Processing, and World Model.

## 1. Core Architecture

The system consists of two parts:
1.  `src/compiler.py`: A compiler that reads a Story YAML file and generates a standalone Python script (the game) AND a companion test script.
2.  **The Game Runtime**: The generated Python code which contains the `World`, `Parser`, `Action` system, and `Rulebooks`.

## 2. Object Model (The "World")

The object hierarchy mimics I7. All game objects inherit from `Entity`.

### Class Hierarchy
*   `Entity` (Base class)
    *   `Room` (Locations)
    *   `Thing` (Physical objects)
        *   `Container` (Can contain things; openable, transparent/opaque)
        *   `Supporter` (Can support things on top)
        *   `Door` (Connects two rooms; lockable)
        *   `Person` (Alive; can carry things; supports conversation)
        *   `Wearable` (Clothing; can be worn)
        *   `Edible` (Food; can be eaten)

### Key Properties (Attributes)
All `Things` have a set of binary flags or properties:
*   `portable`, `lit`, `open`, `openable`, `locked`, `lockable`, `transparent`, `enterable`, `wearable`, `worn`, `edible`.

## 3. Action Processing System

The core of the engine replaces simple `if/else` checks with a Rulebook system.

### The Action Cycle
1.  **Before Rules**: Checks that happen *before* the action is attempted.
2.  **Check Rules**: Logical checks to see if the action is physically possible.
3.  **Carry Out Rules**: The state change happens here.
4.  **After Rules**: Reactions to the successful action.
5.  **Report Rules**: Standard feedback for the user.

### Meta-Commands
*   `undo`: Reverts the game state to the start of the previous turn.
*   `save`: Saves the current game state to a file.
*   `load`: Loads the game state from a file.

## 4. Conversation System
*   **Verbs:** `ask`, `tell`.
*   **Logic:** `Person` entities have a `topics` dictionary mapping keywords to responses.

## 5. How to Add or Update a Story for Testing

The project uses story files not just for content, but as a comprehensive test suite. Each story file can verify specific engine functionalities.

### Location
All story files are located in `stories/yaml/`.

### Naming Convention
*   **File Name:** `<name>.yaml` (e.g., `mystery.yaml`, `containers.yaml`).
*   Avoid using prefixes like `story_` or `test_` unless necessary.

### YAML Structure
The YAML file must include the following metadata to ensure the compiler can generate both the game and the test suite:

```yaml
title: "Feature Test: Containers"
purpose: "Verify open/close/lock mechanics." # Mandatory description

# The 'TEST ME' sequence.
# This list of commands is compiled into an automated test that validates the story.
test_sequence:
  - "look"
  - "open box"
  - "put apple in box"
  - "close box"

# The condition to pass the test
win_condition:
  type: "location"
  target: "Lab"

# Game World Definition
scenes: ...
doors: ...
```

### Purpose of `test_sequence`
The `test_sequence` is crucial. It acts like Inform 7's `TEST ME` command.
*   **Synchronization:** The compiler uses this list to generate `tests/stories/test_<name>.py`. This ensures your test is always in sync with your story definition.
*   **Verification:** Running the test ensures that the sequence of commands is valid and leads to the `win_condition`.

### Compilation & Testing
After creating your file:

1.  **Compile:** `python src/compiler.py stories/yaml/<name>.yaml`
    *   This generates `stories/games/game_<name>.py` and `tests/stories/test_<name>.py`.
2.  **Test:** `python tests/stories/test_<name>.py`

To run the entire suite of stories and tests:
```bash
python src/compiler.py --all
```

### Existing Test Stories
Use these as references or update them to cover new edge cases:
*   `containers.yaml`: Tests containers (open/close, transparent/opaque, putting things in/on).
*   `doors.yaml`: Tests bidirectional doors, keys, and locking mechanics.
*   `undo.yaml`: Tests the `undo` command and state reversion.
*   `save_load.yaml`: Tests persistence via `save` and `load` commands.
*   `supporters.yaml`: Tests 'on' relations (vs 'in').
*   `conversation.yaml`: Tests NPC interactions (`ask`/`tell`).
*   `prison_break.yaml`: A comprehensive integration test combining multiple features.

## 6. AI Integration
The `AIClient` remains a fallback. If the strict parser fails, the AI attempts to map the user's sentence to a strict command format.

## 7. Live AI Testing

Since the AI Client uses paid APIs (e.g., OpenAI), we avoid running live AI calls in the standard automated test suite (mocking is used instead). However, developers should verify the AI integration manually when working on the parser.

**Note:** As of 12/5/2025, `gpt-5-nano` is available and should be used. It requires `temperature: 1`.

### Instructions

1.  **Create a Temporary Story or Script:**
    Create a file named `stories/yaml/live_test.yaml` (or similar), or a python script that imports the generated game. Do NOT commit these files.
    Example content:
    ```yaml
    title: "Live AI Test"
    purpose: "Verify AI fallback."
    scenes:
      - id: "Lab"
        name: "Lab"
        contents:
          - id: "slab"
            name: "slab"
    start_room: "Lab"
    # This command relies on AI to map 'examing' -> 'examine'
    test_sequence:
      - "examing slab"
    win_condition:
      type: "location"
      target: "Lab"
    ```

2.  **Compile the Story:**
    ```bash
    python src/compiler.py stories/yaml/live_test.yaml
    ```

3.  **Run the Game Manually:**
    Do not run the generated test script (it mocks the AI). Run the game directly:
    ```bash
    python stories/games/game_live_test.py
    ```
    Enter commands like `examing slab` and verify the AI output (e.g., `[AI Interpreted: examine slab]`).

4.  **Clean Up:**
    After verification, delete the temporary files to prevent accidental commits:
    ```bash
    rm stories/yaml/live_test.yaml
    rm stories/games/game_live_test.py
    rm tests/stories/test_live_test.py
    ```

## 8. Simulation Testing (NPS)

To evaluate the game's versatility and fun factor, use an AI-driven simulation with Net Promoter Score (NPS).

### Instructions

1.  **Create a Simulation Script:**
    Write a Python script (e.g., `simulate_nps.py`) that imports the generated game class (`World` from `stories/games/game_<name>.py`).
2.  **Implement an AI Persona:**
    Use `gpt-5-nano` (temp 1.0) to act as the player.
    *   **Prompt:** Instruct the AI to explore, deduce verbs, and rate the experience (0-10 NPS).
    *   **Loop:**
        1.  Capture game output.
        2.  Send output + history to Player AI.
        3.  Player AI returns `{ "command": "...", "thought": "...", "nps": 0-10 }`.
        4.  Execute command: `game.parse(command)`.
        5.  Check `game.check_win()`.
3.  **Evaluate:**
    *   High NPS (9-10) indicates the engine correctly handled the player's intent, even with non-standard commands.
    *   Low NPS indicates frustration with the parser or lack of hints.

## 9. Engine Improvement Guide

The engine is designed to be versatile and universal, limiting hard-coded logic.

### Dynamic Verb System
The engine uses reflection to discover available verbs. To add a new verb:

1.  **Story-Specific Verbs (YAML):**
    Simply define the verb in an interaction in your YAML file. The engine will automatically detect it and inform the AI parser.
    ```yaml
    interactions:
      - verb: "calibrate"
        type: "before"
        message: "Calibrating..."
    ```
    *Result:* The AI understands `calibrate [noun]` as a valid command format.

2.  **Standard Verbs (Python):**
    To add a verb globally, add `check_<verb>`, `carry_out_<verb>`, and `report_<verb>` methods to the `Rulebook` class in `src/compiler.py`.
    *   Do not modify a central list of verbs; the engine builds the list dynamically from method names.

### NLP & AI Parser
*   The `AIClient` is automatically fed a vocabulary list that includes all standard verbs and any custom verbs defined in the current story's entities.
*   This allows the AI to map natural language (e.g., "I want to fix the console") to specific custom commands (e.g., "calibrate console") without manual configuration.

### 3. Insights from Simulation
*   **Aliases:** Add broad aliases to objects (e.g., `aliases: ["console", "harmonics", "crystals"]`). This allows commands like "calibrate harmonics" to resolve to the main object naturally.
*   **Graceful Failures:** Ensure custom verbs or `ask` topics return descriptive failure messages (via `self.world.io.write`) and return `False` (handled) rather than relying on generic fallbacks.
*   **Contextual Hints:** The AI parser is sensitive to context. Including hints in descriptions (e.g., "needs calibration") encourages the AI to map ambiguous inputs (like "press") to the desired verb ("calibrate").

### 4. AI & Simulation Best Practices
*   **AI Fallback:** Ensure `dm_config.yaml` is configured and `AIClient` is enabled during simulations to allow natural language processing.
*   **Prompt Engineering:** The LLM system prompt (in `dm_config.yaml`) should explicitly instruct the AI to map "approximate" or "invalid" inputs to valid game commands (e.g., mapping "fix" to "calibrate").
*   **Simulation vs. Testing:**
    *   Use temporary simulation scripts (`simulate_nps.py`) and stories to evaluate "fun" and NLP robustness.
    *   **Delete** simulation artifacts after use.
    *   **Convert** successful capabilities into permanent regression tests (e.g., renaming `alien_contact.yaml` to `dynamic_verbs.yaml` and standardizing the `test_sequence`).
*   **Code Coverage:** Continuously add specific `test_*.yaml` stories to cover engine features (e.g., `wear_eat.yaml`, `enter_exit.yaml`) using Test-Driven Development principles.
